
拓展了DDPO，给出了一种不需要reward model的方法，是在DPO算法上面进行的扩展
DPO的方法

是否是和diffusion-dpo那种同源的数学推理？
