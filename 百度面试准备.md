1.增大感受野的方法
	使用大的卷积核
	增加网络层数
	使用空洞卷积
	
2.交叉熵函数手写
	$$
	CrossEntropy = - \frac{1}{n} \Sigma_{i=0}^n \Sigma_{j=0}^d y_{i,j} log p_{i,j}
	$$
	$$
	BinaryCrossEntropy = - \frac{1}{n} \Sigma_{i=0}^n (y_i log_i + (1-y_i) log (1-p_i)
	$$
3.NMS写法
```python

def nms(dets, thresh):
	x1 = dets[:,:,0]
	y1 = dets[:,:,1]
	x2 = dets[:,:,2]
	y2 = dets[:,:,3]
	confidence = dets[:,:,4]

	order = confidence.argsort()[::-1]
	areas = (y1-y2+1)*(x1-x2+1)
	keep = []
	while(len(order)>0):
		i = order[0]
		xx1 = np.minimum(x1, x1[order[1:]])
		yy1 = np.minimum(y1, y1[order[1:]])
		xx2 = np.minimum(x2, x2[order[1:]])
		yy2 = np.minimum(y2, y2[order[1:]])

		w = np.maximum(0, xx2-xx1+1)
		h = np.maximum(0, yy2-yy1+1)

		iou = w*h / (area[i] + area[order[1:]] - w*h)
		idxs = np.argwhere(iou<=thresh)
		order = order[idxs+1]
		
	return dets[keep]
		
```

5.YOLOv7的模型重参数化
[Yolov6、Yolov7重参数化 RepConv 详解（代码可直接使用）-CSDN博客](https://blog.csdn.net/YXD0514/article/details/132466512?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_baidulandingword~default-1-132466512-blog-126715391.235^v43^pc_blog_bottom_relevance_base2&spm=1001.2101.3001.4242.2&utm_relevant_index=4)

6.YOLOv7模型有哪些改进
[【YOLOv7】主要改进点详解_yolov7改进-CSDN博客](https://blog.csdn.net/weixin_43799388/article/details/126314633)

7.手推BN的前向传播和反向传播
[手推公式之“层归一化”梯度 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/518392433)

8.手撕multihead attention代码
```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiheadAttention(nn.Module):
	def __init__(self, input_dim, head_num, hidden_dim, out_dim):
		super().__init__()
		assert input_dim % head_num == 0
		self.head_num = head_num
		self.head_dim = input_dim // head_num
		self.input_dim = input_dim
		self.hidden_dim = hidden_dim
		self.out_dim = out_dim
		
		self.Q_proj = nn.Linear(head_dim, hidden_dim)
		self.K_proj = nn.Linear(head_dim, hidden_dim)
		self.V_proj = nn.Linear(head_dim, hidden_dim)
		self.out_proj = nn.Linear(hidden_dim * head_num, out_dim)
	def forward(self, x):
		"""
		x: (B,L,C)
		"""
		batch_size = x.size(0)
		seq_length = x.size(1)
		
		x = x.view(batch_size, -1, self.head_num, self.head_dim).permute(0,2,1,3)
		Q = self.Q_proj(x) # (B,head_num,L,hidden_dim)
		K = self.K_proj(x)
		V = self.V_proj(x)

		QK = F.softmax(
		torch.bmm(Q, K.permute(0,1,3,2)) / torch.sqrt(self.head_dim),
		dim = -1
		)
		QKV = torch.bmm(QK, V) # (B,head_num,L,hidden_dim)
		out = out.permute(0, 2, 1, 3).contiguous()
		out = out.view(batch_size, seq_length, -1)
		out = self.out_proj(out)
		return out
		
		
```
9.求解MLE
MLE是最大似然估计，似然函数是$P(X|\theta)$是求解使得似然函数最大的参数

10.求解MAP
MAP是根据贝叶斯公式$P(\theta|X) \propto P(X|\theta)P(\theta)$最大似然估计可以引入参数的先验分布

